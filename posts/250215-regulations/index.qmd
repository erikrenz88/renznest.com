---
title: "Unconstitutionality"
date: "2025-02-15"
categories: [regulations, government, policy]
image: "" 
bluesky-comments:
    enabled: true
freeze: auto
---

```{python}
#| label: set up libraries, logging, and API key
import requests
import logging
import time
import pandas as pd
import gc 
import re
import urllib.parse
import csv
import concurrent
import gdown
import polars as pl
from bs4 import BeautifulSoup
import yaml
import nbformat
from dotenv import load_dotenv
import os
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive

# Configure logging for progress tracking
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

load_dotenv()

# Define API key (replace with your actual API key)
API_KEY = os.getenv('GOV_API')
# Base URL for GovInfo API
GOVINFO_BASE_URL = "https://api.govinfo.gov"
```

```{python}
#| label: python scripting to collect data from govinfo.gov
#| eval: false
def extract_offset_token(next_page_token):
    #Extracts and properly decodes the offsetMark token for pagination.
    if next_page_token.startswith("http"):  
        parsed_url = urllib.parse.urlparse(next_page_token)
        query_params = urllib.parse.parse_qs(parsed_url.query)
        extracted_token = query_params.get("offsetMark", [""])[0]
    else:
        extracted_token = next_page_token  

    return urllib.parse.unquote(extracted_token)

def get_cfr_regulations(start_year=2012, end_year=2025, output_file="cfr_regulations.csv"):
    #Fetches all CFR regulations using pagination, streaming, and incremental file writing.
    headers = {'Accept': 'application/json'}
    
    # Open file for streaming data storage
    with open(output_file, "w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["packageId", "title", "dateIssued"])  # Header

        for year in range(start_year, end_year + 1):
            start_date = f"{year}-01-01T00:00:00Z"
            end_date = f"{year}-12-31T23:59:59Z"
            url = f"{GOVINFO_BASE_URL}/published/{start_date}/{end_date}"
            params = {
                "offsetMark": "*",
                "pageSize": 100,
                "collection": "CFR",
                "api_key": API_KEY
            }

            logger.info(f"Fetching CFR regulations for {year}...")

            while True:
                try:
                    response = requests.get(url, params=params, headers=headers, timeout=30, stream=True)
                    response.raise_for_status()
                    content = response.json()
                    packages = content.get("packages", [])

                    if not packages:
                        logger.warning(f"No packages found for {year}.")
                        break

                    for pkg in packages:
                        writer.writerow([pkg.get("packageId", ""), pkg.get("title", ""), pkg.get("dateIssued", "")])

                    # Pagination Handling
                    if "nextPage" in content and content["nextPage"]:
                        next_offset = extract_offset_token(content["nextPage"])
                        if next_offset:
                            params["offsetMark"] = next_offset
                            time.sleep(1)
                        else:
                            logger.error("Failed to extract offsetMark. Stopping pagination.")
                            break
                    else:
                        break  # No more pages

                except requests.exceptions.RequestException as e:
                    logger.error(f"API request failed for {year}: {e}")
                    break  # Stop processing this year if there's an issue

def get_cfr_agency(package_id):
    #Fetch agency information for a given CFR package from api.govinfo.gov.
    url = f"{GOVINFO_BASE_URL}/packages/{package_id}/summary?api_key={API_KEY}"
    headers = {'Accept': 'application/json'}
    
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        # ✅ Agency information may be in 'organization' or 'agency' fields
        agency = data.get("organization", data.get("agency", "Unknown Agency"))
        return agency
    except requests.exceptions.RequestException as e:
        logger.error(f"Failed to retrieve agency for package {package_id}: {e}")
        return "Unknown Agency"

def get_granules(package_id):
    #Fetches granules (sections) for a given regulation package.
    url = f"{GOVINFO_BASE_URL}/packages/{package_id}/granules?offsetMark=*&pageSize=100&api_key={API_KEY}"
    try:
        response = requests.get(url, headers={'Accept': 'application/json'}, timeout=30)
        response.raise_for_status()
        return response.json().get("granules", [])
    except requests.exceptions.RequestException as e:
        logger.error(f"Failed to retrieve granules for package {package_id}: {e}")
        return []

def get_package_text(package_id):
    #Fetches full text of a regulation in TXT format if available.
    url = f"{GOVINFO_BASE_URL}/packages/{package_id}/summary?api_key={API_KEY}"
    try:
        response = requests.get(url, headers={'Accept': 'application/json'}, timeout=30)
        response.raise_for_status()
        summary = response.json()
        
        if "download" in summary and "txtLink" in summary["download"]:
            txt_url = summary["download"]["txtLink"] + f"?api_key={API_KEY}"
            response = requests.get(txt_url, headers={'Accept': 'text/plain'}, timeout=30)
            response.raise_for_status()
            return response.text
        else:
            return "No text available"
    except requests.exceptions.RequestException as e:
        logger.error(f"Failed to retrieve text for package {package_id}: {e}")
        return "No text available"

def process_regulation_data(input_file="cfr_regulations.csv", output_file="full_regulations.csv"):
    #Processes CFR regulations: retrieves granules, full text, and agency info from api.govinfo.gov.
    results = []
    
    df = pd.read_csv(input_file)

    def fetch_data(row):
        #Helper function for parallel execution.
        package_id = row.packageId
        text = get_package_text(package_id)
        granules = get_granules(package_id)
        
        # ✅ Fetch Agency Name from GovInfo API
        agency_name = get_cfr_agency(package_id)

        return [package_id, row.title, row.dateIssued, agency_name, text, len(granules)]

    logger.info(f"Processing {len(df)} CFR regulations with multi-threading...")

    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        for result in executor.map(fetch_data, df.itertuples(index=False, name="CFRRecord")):
            results.append(result)
            gc.collect()

    with open(output_file, "w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["packageId", "title", "dateIssued", "agency", "text", "granule_count"])
        writer.writerows(results)

    logger.info(f"Completed processing. Data saved to {output_file}")

# get_cfr_regulations(start_year=2012, end_year=2024)  # Fetches regulations and saves to CSV

# process_regulation_data()  # Retrieves granules & text, saves to a new CSV
```

```{python}
#| label: import csv and complete counts
#| cache: true
# Check if the file already exists to avoid re-downloading
file_id = "1Re-xRy9d3jZmWOVjvC4uwyi8UChxzUIY"
output = "large_file.csv"

# Set up authentication from GitHub Secrets
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "credentials.json"

# local dev: load in full_regulations.csv
# regs = pl.read_csv('full_regulations.csv')

# Use gdown with credentials
gdown.download(id=file_id, output=output, quiet=False, fuzzy=True)

if not os.path.exists(output) or os.path.getsize(output) == 0:
    raise Exception("F  ile download failed! Check Google Drive permissions and authentication.")

# Define a lazy loading strategy with chunked reading for better memory management
try:
    regs = pl.scan_csv(output).collect()
except Exception as e:
    raise RuntimeError(f"Failed to read CSV file: {e}")

# Process and collect only necessary columns
if isinstance(regs, pl.LazyFrame):  
    regs = regs.with_columns([
        pl.col("dateIssued").str.to_date().dt.year().alias("year")
    ]).collect() 
else:
    regs = regs.with_columns([
        pl.col("dateIssued").str.to_date().dt.year().alias("year")
    ]) 
```

```{python}
#| label: ChatGPT/Claude assisted analysis
#| cache: true
# Define Bureaucratic & Complexity Term Functions
bureaucratic_terms = ["shall", "must", "require", "submit", "authorize", "comply", "prohibit", "enforce", "mandatory"]
explanatory_terms = ["for the purposes of", "defined as", "background", "explains how"]

def count_bureaucratic_terms(text: str) -> int:
    """Count occurrences of bureaucratic terms in text."""
    return sum(len(re.findall(fr"\b{term}\b", text, re.IGNORECASE)) for term in bureaucratic_terms) if text else 0

def complexity_ratio(text: str) -> float:
    """Calculate complexity ratio based on bureaucratic vs explanatory terms."""
    proc_count = count_bureaucratic_terms(text)
    exp_count = sum(len(re.findall(fr"\b{term}\b", text, re.IGNORECASE)) for term in explanatory_terms) if text else 0
    return proc_count / (exp_count + 1)  # Avoid division by zero

regs = regs.with_columns([
    pl.col("text").map_elements(count_bureaucratic_terms, return_dtype=pl.Int64).alias("bureaucratic_terms"),
    pl.col("text").map_elements(complexity_ratio, return_dtype=pl.Float64).alias("complexity_ratio"),
    pl.col("dateIssued").str.to_date().dt.year().alias("year"),
    pl.when(pl.col("text").is_not_null())
    .then(pl.col("text").str.split(" ").list.len())
    .otherwise(0)
    .alias("reg_word_count"),
])

# Aggregate Data by Title & Year
grouped_regs = regs.group_by(["title", "year"]).agg([
    pl.col("reg_word_count").sum().alias("total_word_count"),
    pl.col("bureaucratic_terms").sum().alias("total_bureaucratic_terms"),
    pl.col("complexity_ratio").mean().alias("avg_complexity_ratio"),
])

# Melt dataframe
meleted_regs = grouped_regs.melt( 
    id_vars = ['title', 'year'],
    variable_name='metric',
    value_name='value'
)
```

```{python}
#| label: Calculate metrics and display results
#| cache: true

# Compute YOY Changes
def percent_change(new, old):
    return ((new - old) / old) * 100 if old != 0 else 0

years = grouped_regs.select("year").unique().sort("year").to_series().to_list()
analysis_results = {}

for i in range(1, len(years)):
    prev_year = years[i - 1]
    curr_year = years[i]

    prev_data = melted_regs.filter(pl.col("year") == prev_year)
    curr_data = melted_regs.filter(pl.col("year") == curr_year)

    prev_word_count = prev_data.filter(pl.col("metric") == "total_word_count")["value"].sum()
    curr_word_count = curr_data.filter(pl.col("metric") == "total_word_count")["value"].sum()

    prev_bureaucratic = prev_data.filter(pl.col("metric") == "total_bureaucratic_terms")["value"].sum()
    curr_bureaucratic = curr_data.filter(pl.col("metric") == "total_bureaucratic_terms")["value"].sum()

    prev_complexity = prev_data.filter(pl.col("metric") == "avg_complexity_ratio")["value"].mean()
    curr_complexity = curr_data.filter(pl.col("metric") == "avg_complexity_ratio")["value"].mean()

    word_growth = percent_change(curr_word_count, prev_word_count)
    bureaucratic_change = percent_change(curr_bureaucratic, prev_bureaucratic)
    complexity_change = percent_change(curr_complexity, prev_complexity)

    efficiency_score = (bureaucratic_change + complexity_change) / (2 * word_growth) if word_growth != 0 else 0

    analysis_results[curr_year] = {
        "word_growth": word_growth,
        "bureaucratic_change": bureaucratic_change,
        "complexity_change": complexity_change,
        "efficiency_score": efficiency_score
    }

df_results = pl.DataFrame(
    [{"year": year, **metrics} for year, metrics in analysis_results.items()]
)

# Fetch Laws Passed Per Year
congress_map = {
    2012: [112], 2013: [113], 2014: [113],
    2015: [114], 2016: [114],
    2017: [115], 2018: [115],
    2019: [116], 2020: [116],
    2021: [117], 2022: [117],
    2023: [118], 2024: [118]
}

laws_by_congress = {
    112: 284, 113: 296, 114: 329, 115: 442,
    116: 344, 117: 328, 118: 280
}

laws_by_year = {year: sum(laws_by_congress.get(congress, 0) for congress in congresses) // 2 for year, congresses in congress_map.items()}

# Compute Additional Metrics
for year in sorted(laws_by_year.keys()):
    year_filtered = regs.filter(pl.col("year") == year)
    num_sections = year_filtered.shape[0]  # Assuming granule count is based on number of sections
    total_word_count = year_filtered["reg_word_count"].sum()
    total_bureaucratic_terms = year_filtered["bureaucratic_terms"].sum()
    num_laws_passed = laws_by_year.get(year, 1)
    unconstitutionality = num_sections / num_laws_passed if num_laws_passed else 0

    df_results = df_results.with_columns([
        pl.lit(num_sections).alias("num_sections"),
        pl.lit(num_laws_passed).alias("num_laws_passed"),
        pl.lit(total_word_count).alias("total_word_count"),
        pl.lit(total_bureaucratic_terms).alias("total_bureaucratic_terms"),
        pl.lit(unconstitutionality).alias("unconstitutionality_index"),
    ])

# Load DOGE Data for Comparison
try:
    doge = pl.read_csv("posts/250215-regulations/regulation-data.csv")
    joined = df_results.join(doge, on="year", how="inner")
except:
    print("DOGE data file not available for comparison")

# Prepare Data for Visualization
df_melted = df_results.unpivot(
    id_vars=["year"], 
    value_vars=["num_laws_passed", "num_sections"], 
).rename({"variable": "count_type", "value": "value"})  

# Convert to dictionary format (array of objects)
df_melted_json = df_melted.to_dicts()
melted_regs_json = melted_regs.to_dicts()
yoy_json = df_results.to_dicts()

# Send to OJS
ojs_define(data=df_melted_json)
ojs_define(int=melted_regs_json)
ojs_define(yoy=yoy_json)
```

```{ojs}
Plot = await import('https://cdn.jsdelivr.net/npm/@observablehq/plot@0.6.16/+esm')

// Ensure date formatting for visualization
dataFormatted = data.map(d => ({...d, year: new Date(d.year, 0, 1)}))

// Grouped column chart (Rules & Laws per year)
Plot.plot({
  marginLeft: 80,
  marginBottom: 40,
  marginRight: 80,
  marginTop: 40,
  width: 960,
  height: 500,
  x: {label: 'Year', tickFormat: d3.utcFormat("%Y")},
  y: {label: 'Count'},
  color: {legend: true, scheme: "tableau10"},
  marks: [
    Plot.barY(dataFormatted, {x: 'year', y: 'value', fill: 'count_type', tip: true}),
    Plot.ruleY([0])
  ]
})
```

```{ojs}
// DOGE Comparison (Word Counts vs. Sections of Regulation)
Plot.plot({
  marginLeft: 80,
  marginBottom: 40,
  marginRight: 80,
  marginTop: 40,
  width: 960,
  height: 500,
  x: {label: 'Year', tickFormat: d3.utcFormat("%Y")},
  y: {label: 'Word Count'},
  color: {legend: true, scheme: "set2"},
  marks: [
    Plot.barY(dataFormatted, {x: 'year', y: 'value', fill: 'count_type', tip: true}),
    Plot.ruleY([0])
  ]
})
```

```{ojs}
// Line Chart for Word Count by Agency over Time (With Checkboxes)
viewof agency = Inputs.checkbox(
    [...new Set(int.map(d => d.title))],
    {label: "Select Agencies"}
)

filteredData = int.filter(d => agency.includes(d.title)).map(d => ({...d, year: new Date(d.year, 0, 1)}))

Plot.plot({
  marginLeft: 80,
  marginBottom: 40,
  marginRight: 80,
  marginTop: 40,
  width: 960,
  height: 500,
  x: {label: 'Year', tickFormat: d3.utcFormat("%Y")},
  y: {label: 'Word Count'},
  color: {legend: true, scheme: "category10"},
  marks: [
    Plot.line(filteredData, {x: 'year', y: 'value', stroke: 'title', tip: true}),
    Plot.ruleY([0])
  ]
})
```

```{ojs}
// YOY Change Chart (Comparison of Word Growth, Bureaucratic Change, Complexity Change)
yoyFormatted = yoy.map(d => ({...d, year: new Date(d.year, 0, 1)}))

Plot.plot({
  marginLeft: 80,
  marginBottom: 40,
  marginRight: 80,
  marginTop: 40,
  width: 960,
  height: 500,
  x: {label: 'Year', tickFormat: d3.utcFormat("%Y")},
  y: {label: 'Percent Change'},
  color: {legend: true, scheme: "dark2"},
  marks: [
    Plot.line(yoyFormatted, {x: 'year', y: 'word_growth', stroke: 'Word Growth', tip: true}),
    Plot.line(yoyFormatted, {x: 'year', y: 'bureaucratic_change', stroke: 'Bureaucratic Change', tip: true}),
    Plot.line(yoyFormatted, {x: 'year', y: 'complexity_change', stroke: 'Complexity Change', tip: true}),
    Plot.ruleY([0])
  ]
})
```

### Background
The Department of Government Efficiency (laughably/ironically/disrespectfully, [DOGE](https://en.wikipedia.org/wiki/Doge_(meme))) was created by [Exeutive Order](https://www.whitehouse.gov/presidential-actions/2025/01/establishing-and-implementing-the-presidents-department-of-government-efficiency/) on Jan. 20 2025. The purpose is defined as:
> "This Executive Order establishes the Department of Government Efficiency to implement the President’s DOGE Agenda, by modernizing Federal technology and software to maximize governmental efficiency and productivity."

Currently, there is ambiguity as to the DOGE administrator as shown in the various sources and quotes in the [Wikipedia article](https://en.wikipedia.org/wiki/Department_of_Government_Efficiency#:~:text=President%20Donald%20Trump%20confirmed%20Elon,required%20of%20full%20time%20employees.):
> "Trump has said that businessman Elon Musk is "in charge" of DOGE, but the White House has denied that Musk is a DOGE administrator or DOGE employee,[9][2][10] and said Musk "has no actual or formal authority to make government decisions"."

The contracted organization-the true status of the organization, not actually a department of the U.S. government-released the first version of their website recently (Feb. 12 2025) attempting to make their effiency findings transparent through monies saved and an assessment of bueraucractic overreach. Many reports of the flaws in their savings analyses have been publicized but a concerning set of analysis is their regulations page.

Seemingly, the purpose of the page is to look at the amount of regulation (rules from agencies not created by congress) compared to the laws passed by Congress. That is to say there is more government rule making than congressional (representing the people's interests). The main metrics used are word counts by agency and year, and the Unconstitutionality Index.

#### Unconstitutionally Index
This index was created by the (Competitive Enterprise Institude)[https://cei.org/opeds_articles/the-2025-unconstitutionality-index-exposing-congresss-abdication-of-power/], a nonprofit advocating for "regulatory reform on a wide range of policy issues". While a valid index, it should be treated as such-a tool to measure change in a group of representative data. It can provide a simple metric to track but does not provide full context. 

It is included in this analysis to compare the various metrics being used to asses regulatory reach. The discussion will include why metrics can only represent and should not be removed from context.

Here, these metrics are reviewed and compared to other metrics created for the purpose of this analysis with idea generation and code assistance from generative AI tools (Claude/ChatGPT) which will be flagged where used. All code and data will be available open source for reproducibility and transparency.

### Methods

#### Regulations
Regulations were pulled from the GovInfo.gov API. All regulations were pulled between 2012 and 2014, looking for titles of regulations, the issued date of the regulation, and a package ID used to identify regulations and their details. Once regulations were pulled, granules (details for each regulation record) were pulled to obtain the agency that produced the regulation and text for each regulation.

These were saved to .csv file stored on Google Drive due to size restrictions on GitHub. The .csv was loaded into python and the following metrics were calculated:
    - word count: count of all individual words within the full text of the regulation;
    - bueraucractic terms: count of all terms that described bueraucratic action ("shall", "must", "require", "submit", "authorize", "comply", "prohibit", "enforce", "mandatory"; note this list is not exhaustive but representative);
    - complexity ratio: ratio of bureuacratic terms to explanatory terms ("for the purposes of", "defined as", "background", "explains how"; note this list is not exhaustive but representative)

Percent changes year over year were calculated as:
$$
\text{Percent Change} = \frac{year_{new} - year_{old}}{year_{old}} \times 100
$$

The next calculation was the unconstitutionality index but requires numbers of laws by year. The method for gathering these is defined next.

#### Laws
Laws were counted from Congress.gov using the search feature for "Laws" between 2012 - 2024. Under "Legislative Action", "Laws Enacted" was selected. The specific congresses were selected by their year span and the years were mapped to congressional sessions. While innaccurate, laws were split evenly by the years of the congressional sessions (divided by 2) for a quick analysis. Improvements would be to manually count for each year that laws were passed but there is currently no automated way of collecting this data.

#### Unconstitutionally Index
$$
\text{Unconstitutionality Index} = \frac{n_{regulations}}{{n_{laws}}}
$$

### Results

#### Analysis findings


#### Comparison to DOGE results
```{python}
doge = pd.read_csv('regulation-data.csv', index_col=None)
```

### Discussion 

#### Importance of context


#### Importance of transparency


#### Lack of expertise


### Conclusion
