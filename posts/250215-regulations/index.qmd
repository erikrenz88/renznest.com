---
title: "Unconstitutionality"
date: "2025-02-15"
categories: [regulations, government, policy]
image: "" 
bluesky-comments:
    enabled: true
freeze: auto
---
```{r}
pacman::p_load(
    'arrow',
    'dplyr',
    'readr',
    'tidyr',
    'stringr',
    'reticulate'
)
```

```{python}
#| label: set up libraries, logging, and API key
import requests
import logging
import time
import pandas as pd
import gc 
import re
import urllib.parse
import csv
import concurrent
import gdown
import polars as pl
from bs4 import BeautifulSoup
import yaml
import nbformat
from dotenv import load_dotenv
import os
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive

# Configure logging for progress tracking
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

load_dotenv()

# Define API key (replace with your actual API key)
API_KEY = os.getenv('GOV_API')
# Base URL for GovInfo API
GOVINFO_BASE_URL = "https://api.govinfo.gov"
```

```{python}
#| label: python scripting to collect data from govinfo.gov
#| eval: false
def extract_offset_token(next_page_token):
    #Extracts and properly decodes the offsetMark token for pagination.
    if next_page_token.startswith("http"):  
        parsed_url = urllib.parse.urlparse(next_page_token)
        query_params = urllib.parse.parse_qs(parsed_url.query)
        extracted_token = query_params.get("offsetMark", [""])[0]
    else:
        extracted_token = next_page_token  

    return urllib.parse.unquote(extracted_token)

def get_cfr_regulations(start_year=2012, end_year=2025, output_file="cfr_regulations.csv"):
    #Fetches all CFR regulations using pagination, streaming, and incremental file writing.
    headers = {'Accept': 'application/json'}
    
    # Open file for streaming data storage
    with open(output_file, "w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["packageId", "title", "dateIssued"])  # Header

        for year in range(start_year, end_year + 1):
            start_date = f"{year}-01-01T00:00:00Z"
            end_date = f"{year}-12-31T23:59:59Z"
            url = f"{GOVINFO_BASE_URL}/published/{start_date}/{end_date}"
            params = {
                "offsetMark": "*",
                "pageSize": 100,
                "collection": "CFR",
                "api_key": API_KEY
            }

            logger.info(f"Fetching CFR regulations for {year}...")

            while True:
                try:
                    response = requests.get(url, params=params, headers=headers, timeout=30, stream=True)
                    response.raise_for_status()
                    content = response.json()
                    packages = content.get("packages", [])

                    if not packages:
                        logger.warning(f"No packages found for {year}.")
                        break

                    for pkg in packages:
                        writer.writerow([pkg.get("packageId", ""), pkg.get("title", ""), pkg.get("dateIssued", "")])

                    # Pagination Handling
                    if "nextPage" in content and content["nextPage"]:
                        next_offset = extract_offset_token(content["nextPage"])
                        if next_offset:
                            params["offsetMark"] = next_offset
                            time.sleep(1)
                        else:
                            logger.error("Failed to extract offsetMark. Stopping pagination.")
                            break
                    else:
                        break  # No more pages

                except requests.exceptions.RequestException as e:
                    logger.error(f"API request failed for {year}: {e}")
                    break  # Stop processing this year if there's an issue

def get_cfr_agency(package_id):
    #Fetch agency information for a given CFR package from api.govinfo.gov.
    url = f"{GOVINFO_BASE_URL}/packages/{package_id}/summary?api_key={API_KEY}"
    headers = {'Accept': 'application/json'}
    
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        # ✅ Agency information may be in 'organization' or 'agency' fields
        agency = data.get("organization", data.get("agency", "Unknown Agency"))
        return agency
    except requests.exceptions.RequestException as e:
        logger.error(f"Failed to retrieve agency for package {package_id}: {e}")
        return "Unknown Agency"

def get_granules(package_id):
    #Fetches granules (sections) for a given regulation package.
    url = f"{GOVINFO_BASE_URL}/packages/{package_id}/granules?offsetMark=*&pageSize=100&api_key={API_KEY}"
    try:
        response = requests.get(url, headers={'Accept': 'application/json'}, timeout=30)
        response.raise_for_status()
        return response.json().get("granules", [])
    except requests.exceptions.RequestException as e:
        logger.error(f"Failed to retrieve granules for package {package_id}: {e}")
        return []

def get_package_text(package_id):
    #Fetches full text of a regulation in TXT format if available.
    url = f"{GOVINFO_BASE_URL}/packages/{package_id}/summary?api_key={API_KEY}"
    try:
        response = requests.get(url, headers={'Accept': 'application/json'}, timeout=30)
        response.raise_for_status()
        summary = response.json()
        
        if "download" in summary and "txtLink" in summary["download"]:
            txt_url = summary["download"]["txtLink"] + f"?api_key={API_KEY}"
            response = requests.get(txt_url, headers={'Accept': 'text/plain'}, timeout=30)
            response.raise_for_status()
            return response.text
        else:
            return "No text available"
    except requests.exceptions.RequestException as e:
        logger.error(f"Failed to retrieve text for package {package_id}: {e}")
        return "No text available"

def process_regulation_data(input_file="cfr_regulations.csv", output_file="full_regulations.csv"):
    #Processes CFR regulations: retrieves granules, full text, and agency info from api.govinfo.gov.
    results = []
    
    df = pd.read_csv(input_file)

    def fetch_data(row):
        #Helper function for parallel execution.
        package_id = row.packageId
        text = get_package_text(package_id)
        granules = get_granules(package_id)
        
        # ✅ Fetch Agency Name from GovInfo API
        agency_name = get_cfr_agency(package_id)

        return [package_id, row.title, row.dateIssued, agency_name, text, len(granules)]

    logger.info(f"Processing {len(df)} CFR regulations with multi-threading...")

    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        for result in executor.map(fetch_data, df.itertuples(index=False, name="CFRRecord")):
            results.append(result)
            gc.collect()

    with open(output_file, "w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["packageId", "title", "dateIssued", "agency", "text", "granule_count"])
        writer.writerows(results)

    logger.info(f"Completed processing. Data saved to {output_file}")

# get_cfr_regulations(start_year=2012, end_year=2024)  # Fetches regulations and saves to CSV

# process_regulation_data()  # Retrieves granules & text, saves to a new CSV
```

```{python}
#| label: import csv and complete counts
#| cache: true
# Check if the file already exists to avoid re-downloading
file_id = "1Re-xRy9d3jZmWOVjvC4uwyi8UChxzUIY"
output = "large_file.csv"

# Set up authentication from GitHub Secrets
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "credentials.json"

# local dev: load in full_regulations.csv
# regs = pl.read_csv('full_regulations.csv')

# Use gdown with credentials
gdown.download(id=file_id, output=output, quiet=False, fuzzy=True)

if not os.path.exists(output) or os.path.getsize(output) == 0:
    raise Exception("File download failed! Check Google Drive permissions and authentication.")

# Define a lazy loading strategy with chunked reading for better memory management
try:
    regs = pl.scan_csv(output).collect()
except Exception as e:
    raise RuntimeError(f"Failed to read CSV file: {e}")

# Process and collect only necessary columns
if isinstance(regs, pl.LazyFrame):  
    regs = regs.with_columns([
        pl.col("dateIssued").str.to_date().dt.year().alias("year")
    ]).collect() 
else:
    regs = regs.with_columns([
        pl.col("dateIssued").str.to_date().dt.year().alias("year")
    ])

regs_pandas = regs.to_pandas()
```

```{r}
library(reticulate)
library(dplyr)
library(readr)
library(tidyr)
library(stringr)

# Access the Pandas DataFrame from Python
regs <- py$regs_pandas

# Access list of rules
rules <- read_csv('posts/250215-regulations/federal_register_rule_counts.csv')

# Access DOGE data
doge <- read_csv(doge_data_path)
```

```{r}
# Define bureaucratic and explanatory terms
bureaucratic_terms <- c("shall", "must", "require", "submit", "authorize", "comply", "prohibit", "enforce", "mandatory")
explanatory_terms <- c("for the purposes of", "defined as", "background", "explains how")

# Function to count bureaucratic terms in text
count_bureaucratic_terms <- function(text) {
  if (is.na(text)) return(0)
  sum(sapply(bureaucratic_terms, function(term) str_count(text, fixed(term, ignore_case = TRUE))))
}

# Function to calculate complexity ratio
complexity_ratio <- function(text) {
  if (is.na(text)) return(0)
  proc_count <- count_bureaucratic_terms(text)
  exp_count <- sum(sapply(explanatory_terms, function(term) str_count(text, fixed(term, ignore_case = TRUE))))
  return(proc_count / (exp_count + 1))  # Avoid division by zero
}

# Ensure dateIssued column is in Date format and extract year
regs <- regs %>%
  mutate(
    year = as.integer(format(as.Date(dateIssued, format = "%Y-%m-%d"), "%Y")),
    bureaucratic_terms = sapply(text, count_bureaucratic_terms),
    complexity_ratio = sapply(text, complexity_ratio),
    reg_word_count = ifelse(!is.na(text), str_count(text, "\\S+"), 0)  # Count words
  ) |>
  ungroup()
```

```{r}
# Aggregate data by Title & Year
grouped_regs <- regs %>% 
  group_by(agency, year) %>%
  summarise(
    num_sections = sum(granule_count, na.rm = TRUE),
    total_word_count = sum(reg_word_count, na.rm = TRUE),
    total_bureaucratic_terms = sum(bureaucratic_terms, na.rm = TRUE),
    avg_complexity_ratio = mean(complexity_ratio, na.rm = TRUE),
    .groups = "drop"
  )
```

```{r}
# Melt dataframe (equivalent to pivot_longer in R)
melted_regs <- grouped_regs %>%
  pivot_longer(cols = c(total_word_count, total_bureaucratic_terms, avg_complexity_ratio),
               names_to = "metric",
               values_to = "value")

# Function for percentage change calculation
percent_change <- function(new, old) {
  if (old == 0) return(0)
  return((new - old) / old * 100)
}
```

```{r}
# Laws passed per year mapping
congress_map <- list(
  "2012" = c(112), "2013" = c(113), "2014" = c(113),
  "2015" = c(114), "2016" = c(114),
  "2017" = c(115), "2018" = c(115),
  "2019" = c(116), "2020" = c(116),
  "2021" = c(117), "2022" = c(117),
  "2023" = c(118), "2024" = c(118)
)

laws_by_congress <- c(
  "112" = 284, "113" = 296, "114" = 329, "115" = 442,
  "116" = 344, "117" = 328, "118" = 280
)

# Compute laws passed per year
laws_by_year <- sapply(names(congress_map), function(year) {
  sum(sapply(congress_map[[year]], function(congress) laws_by_congress[as.character(congress)]), na.rm = TRUE) / 2
}, simplify = TRUE)

laws_by_year <- data.frame(year = as.integer(names(laws_by_year)), num_laws_passed = laws_by_year)

# load DOGE data
doge_data_path <- "posts/250215-regulations/regulation-data.csv"

if (file.exists(doge_data_path)) {
   doge <- read_csv(doge_data_path)
#  df_results <- df_results %>%
#   inner_join(doge, by = "year")
 } else {  
   print("DOGE data file not available for comparison")
}

# set up yearly section and word counts then join yearly laws and DOGE data
df_sect <- grouped_regs |>
  left_join(laws_by_year, by = 'year') |>
  left_join(rules, by = 'year') |>
  mutate(
    unconstitutionality_index = rule_count / num_laws_passed
  ) |>
  left_join(doge, by = 'year') |>
  rename(DOGE_word_count = word_count) |>
  pivot_longer(
    cols = c('total_bureaucratic_terms', 'avg_complexity_ratio', 'total_word_count', 'rule_count', 'num_laws_passed', 'num_sections', 'unconstitutionality_index', 'DOGE_word_count', 'regulation_count'),
    names_to = 'metric',
    values_to = 'value'
  )

# charts
charts <- df_sect |>
  select(agency, year, metric, value) |>
  mutate(year = as.character(year))

# laws and rules charts
lawrule <- laws_by_year |>
  inner_join(rules, by = 'year') |>
  pivot_longer(
    cols = c('num_laws_passed', 'rule_count'),
    names_to = 'metric',
    values_to = 'value'
  )

ojs_define(lawrule = lawrule)
ojs_define(data = charts)
```

```{r}
#| eval: false
# Prepare Data for Visualization
#df_melted <- df_results %>%
#  pivot_longer(cols = c(word_growth, bureaucratic_change, complexity_change, efficiency_score),
#               names_to = "metric_type",
#               values_to = "value")

```

### Background
The Department of Government Efficiency (laughably/ironically/disrespectfully, [DOGE](https://en.wikipedia.org/wiki/Doge_(meme))) was created by [Exeutive Order](https://www.whitehouse.gov/presidential-actions/2025/01/establishing-and-implementing-the-presidents-department-of-government-efficiency/) on Jan. 20 2025. The purpose is defined as:
> "This Executive Order establishes the Department of Government Efficiency to implement the President’s DOGE Agenda, by modernizing Federal technology and software to maximize governmental efficiency and productivity."

Currently, there is ambiguity as to the DOGE administrator as shown in the various sources and quotes in the [Wikipedia article](https://en.wikipedia.org/wiki/Department_of_Government_Efficiency#:~:text=President%20Donald%20Trump%20confirmed%20Elon,required%20of%20full%20time%20employees.):
> "Trump has said that businessman Elon Musk is "in charge" of DOGE, but the White House has denied that Musk is a DOGE administrator or DOGE employee,[9][2][10] and said Musk "has no actual or formal authority to make government decisions"."

The contracted organization-the true status of the organization, not actually a department of the U.S. government-released the first version of their website recently (Feb. 12 2025) attempting to make their effiency findings transparent through monies saved and an assessment of bueraucractic overreach. Many reports of the flaws in their savings analyses have been publicized but a concerning set of analysis is their regulations page.

Seemingly, the purpose of the page is to look at the amount of regulation (rules from agencies not created by congress) compared to the laws passed by Congress. That is to say there is more government rule making than congressional (representing the people's interests). The main metrics used are word counts by agency and year, and the Unconstitutionality Index.

#### Unconstitutionally Index
This index was created by the (Competitive Enterprise Institude)[https://cei.org/opeds_articles/the-2025-unconstitutionality-index-exposing-congresss-abdication-of-power/], a nonprofit advocating for "regulatory reform on a wide range of policy issues". While a valid index, it should be treated as such-a tool to measure change in a group of representative data. It can provide a simple metric to track but does not provide full context. 

It is included in this analysis to compare the various metrics being used to asses regulatory reach. The discussion will include why metrics can only represent and should not be removed from context.

Here, these metrics are reviewed and compared to other metrics created for the purpose of this analysis with idea generation and code assistance from generative AI tools (Claude/ChatGPT) which will be flagged where used. All code and data will be available open source for reproducibility and transparency.

### Methods

#### Regulations
Regulations were pulled from the GovInfo.gov API. All regulations were pulled between 2012 and 2014, looking for titles of regulations, the issued date of the regulation, and a package ID used to identify regulations and their details. Once regulations were pulled, granules (details for each regulation record) were pulled to obtain the agency that produced the regulation and text for each regulation.

These were saved to .csv file stored on Google Drive due to size restrictions on GitHub. The .csv was loaded into python and the following metrics were calculated:
    - word count: count of all individual words within the full text of the regulation;
    - bueraucractic terms: count of all terms that described bueraucratic action ("shall", "must", "require", "submit", "authorize", "comply", "prohibit", "enforce", "mandatory"; note this list is not exhaustive but representative);
    - complexity ratio: ratio of bureuacratic terms to explanatory terms ("for the purposes of", "defined as", "background", "explains how"; note this list is not exhaustive but representative)

Percent changes year over year were calculated as:
$$
\text{Percent Change} = \frac{year_{new} - year_{old}}{year_{old}} \times 100
$$

The next calculation was the unconstitutionality index but requires numbers of laws by year. The method for gathering these is defined next.

#### Laws
Laws were counted from Congress.gov using the search feature for "Laws" between 2012 - 2024. Under "Legislative Action", "Laws Enacted" was selected. The specific congresses were selected by their year span and the years were mapped to congressional sessions. While innaccurate, laws were split evenly by the years of the congressional sessions (divided by 2) for a quick analysis. Improvements would be to manually count for each year that laws were passed but there is currently no automated way of collecting this data.

#### Unconstitutionally Index
$$
\text{Unconstitutionality Index} = \frac{n_{regulations}}{{n_{laws}}}
$$

### Results

#### Analysis findings
Below is the number of rules and laws by year done with the above method.
```{ojs}
Plot = import('https://cdn.jsdelivr.net/npm/@observablehq/plot@0.6.16/+esm')
d3 = require("d3@7")

tlr = transpose(lawrule)

// Ensure date formatting for visualization
tlrFormatted = tlr.map(d => ({...d, year: new Date(d.year, 0, 1)}))

// Grouped column chart (Rules & Laws per year per agency)
Plot.plot({
  marginLeft: 80,
  marginBottom: 40,
  marginRight: 80,
  marginTop: 40,
  width: 960,
  height: 500,
  fx: {label: 'Year', tickFormat: d3.utcFormat("%Y")},
  x: {axis: null, domain: ['rule_count', 'num_laws_passed']},
  y: {label: 'Count'},
  color: {
    legend: true, 
    domain: ['num_laws_passed', 'rule_count'],
    range: ['#345995', '#03CEA4'],
    tickFormat: text => text.replace(/_/g, " ").replace(/^./, str => str.toUpperCase()) 
  },
  marks: [
    Plot.barY(tlrFormatted, {fx: 'year', x: 'metric', y: 'value', fill: 'metric', tip: true}),
    Plot.ruleY([0])
  ]
})
```

The next chart compares DOGE word counts by year to the above methods word count by year.
```{ojs}
//| echo: false 
//| warning: false 
//| message: false 
tdata = transpose(data)

// Ensure date formatting for visualization
dataFormatted = tdata.map(d => ({...d, year: new Date(d.year, 0, 1)}))

rolledUpData = Array.from(
  d3.rollup(
    dataFormatted, 
    v => d3.sum(v, d => d.value), 
    d => d.year.getTime(),  // Use timestamp for grouping
    d => d.agency,
    d => d.metric  // Ensure metric is maintained
  ), 
  ([year, agencies]) => 
    Array.from(agencies, ([agency, metrics]) => 
      Array.from(metrics, ([metric, value]) => ({year: new Date(year), agency, metric, value}))
    )
).flat(2)  // Flatten nested array structure

// get unique Agencies
uniqueAgencies = [...new Set(rolledUpData.map(d => d.agency))];
uniqueAgencies.unshift('All')

// Checkbox input for agency selection
viewof agency = Inputs.checkbox(uniqueAgencies, {
    label: "Select Agencies",
    value: ["All"]  // Default to selecting all
})

filtered = rolledUpData.filter(d => 
  agency.includes('All') || agency.includes(d.agency)
)

chart1 = filtered.filter(d => ['total_word_count', 'DOGE_word_count'].includes(d.metric))

chart2 = filtered.filter(d => ['avg_complexity_ratio', 'unconstitutionality_index'].includes(d.metric))                    

// bar chart (word count by agency)
Plot.plot({
  marginLeft: 80,
  marginBottom: 40,
  marginRight: 80,
  marginTop: 40,
  width: 960,
  height: 500,
  fx: {label: 'Year', tickFormat: d3.utcFormat("%Y")},
  x: {axis: null, label: 'Year', tickFormat: d3.utcFormat("%Y")},
  y: {label: 'Regulatory Metric'},
  color: {
    domain: ["total_word_count", "DOGE_word_count"],
    range: ["#EAC435", "#345995"],
    legend: true,
    tickFormat: text => text.replace(/_/g, " ").replace(/^./, str => str.toUpperCase()) 
  },
  marks: [
    Plot.barY(chart1, {fx: 'year', x: 'metric', y: 'value', fill: 'metric', tip: true}),
    Plot.ruleY([0])
  ]
})
```

As these seemed to fall short, the next charts look at various metrics (produced with help from ChatGPT Data Analyst (4o) and Claude Sonnet 3.5) that look at things like bureaucracy, efficiency, and word counts by agency by year, as well as year over year changes. The charts can be filtered by all agencies (titles in the data).

```{ojs}
// Grouped line chart (complexity and unconstitutionality index by agency)
Plot.plot({
  marginLeft: 80,
  marginBottom: 40,
  marginRight: 80,
  marginTop: 40,
  width: 960,
  height: 500,
  x: {label: 'Year', tickFormat: d3.utcFormat("%Y")},
  y: {label: 'Regulatory Metric'},
  color: {
    domain: ["avg_complexity_ratio", "unconstitutionality_index"],
    range: ["#1C949D", "#FB4D3D"],
    legend: true,
    tickFormat: text => text.replace(/_/g, " ").replace(/^./, str => str.toUpperCase()) 
  },
  marks: [
    Plot.line(chart2, {x: 'year', y: 'value', stroke: 'metric', tip: true}),
    Plot.ruleY([0])
  ]
})
```

### Discussion 

#### Importance of context


#### Importance of transparency


#### Lack of expertise


### Conclusion
